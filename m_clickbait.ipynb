{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/rsc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/rsc/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Code source: https://degravek.github.io/project-pages/project1/2017/04/28/New-Notebook/\n",
    "# Code modified by Renato\n",
    "# Dataset from Chakraborty et al. (https://github.com/bhargaviparanjape/clickbait/tree/master/dataset)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clickbait():\n",
    "    \n",
    "    question_words = ['who', 'whos', 'whose', 'what', 'whats', 'whatre', 'when', 'whenre', 'whens', 'couldnt',\n",
    "            'where', 'wheres', 'whered', 'why', 'whys', 'can', 'cant', 'could', 'will', 'would', 'is',\n",
    "            'isnt', 'should', 'shouldnt', 'you', 'your', 'youre', 'youll', 'youd', 'here', 'heres',\n",
    "            'how', 'hows', 'howd', 'this', 'are', 'arent', 'which', 'does', 'doesnt']\n",
    "\n",
    "    contractions = ['tis', 'aint', 'amnt', 'arent', 'cant', 'couldve', 'couldnt', 'couldntve',\n",
    "                    'didnt', 'doesnt', 'dont', 'gonna', 'gotta', 'hadnt', 'hadntve', 'hasnt',\n",
    "                    'havent', 'hed', 'hednt', 'hedve', 'hell', 'hes', 'hesnt', 'howd', 'howll',\n",
    "                    'hows', 'id', 'idnt', 'idntve', 'idve', 'ill', 'im', 'ive', 'ivent', 'isnt',\n",
    "                    'itd', 'itdnt', 'itdntve', 'itdve', 'itll', 'its', 'itsnt', 'mightnt',\n",
    "                    'mightve', 'mustnt', 'mustntve', 'mustve', 'neednt', 'oclock', 'ol', 'oughtnt',\n",
    "                    'shant', 'shed', 'shednt', 'shedntve', 'shedve', 'shell', 'shes', 'shouldve',\n",
    "                    'shouldnt', 'shouldntve', 'somebodydve', 'somebodydntve', 'somebodys',\n",
    "                    'someoned', 'someonednt', 'someonedntve', 'someonedve', 'someonell', 'someones',\n",
    "                    'somethingd', 'somethingdnt', 'somethingdntve', 'somethingdve', 'somethingll',\n",
    "                    'somethings', 'thatll', 'thats', 'thatd', 'thered', 'therednt', 'theredntve',\n",
    "                    'theredve', 'therere', 'theres', 'theyd', 'theydnt', 'theydntve', 'theydve',\n",
    "                    'theydvent', 'theyll', 'theyontve', 'theyre', 'theyve', 'theyvent', 'wasnt',\n",
    "                    'wed', 'wedve', 'wednt', 'wedntve', 'well', 'wontve', 'were', 'weve', 'werent',\n",
    "                    'whatd', 'whatll', 'whatre', 'whats', 'whatve', 'whens', 'whered', 'wheres',\n",
    "                    'whereve', 'whod', 'whodve', 'wholl', 'whore', 'whos', 'whove', 'whyd', 'whyre',\n",
    "                    'whys', 'wont', 'wontve', 'wouldve', 'wouldnt', 'wouldntve', 'yall', 'yalldve',\n",
    "                    'yalldntve', 'yallll', 'yallont', 'yallllve', 'yallre', 'yallllvent', 'yaint',\n",
    "                    'youd', 'youdve', 'youll', 'youre', 'yourent', 'youve', 'youvent']\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        result = text.replace('/', '').replace('\\n', '')\n",
    "        result = re.sub(r'[1-9]+', 'number', result)\n",
    "        result = re.sub(r'(\\w)(\\1{2,})', r'\\1', result)\n",
    "        result = re.sub(r'(?x)\\b(?=\\w*\\d)\\w+\\s*', '', result)\n",
    "        result = ''.join(t for t in result if t not in punctuation)\n",
    "        result = re.sub(r' +', ' ', result).lower().strip()\n",
    "        return result\n",
    "    \n",
    "    def cnt_stop_words(self, text):\n",
    "        s = text.split()\n",
    "        num = len([word for word in s if word in self.stop])\n",
    "        return num\n",
    "\n",
    "    def num_contract(self, text):\n",
    "        s = text.split()\n",
    "        num = len([word for word in s if word in self.contractions])\n",
    "        return num\n",
    "\n",
    "    def question_word(self, text):\n",
    "        s = text.split()\n",
    "        if s[0] in self.question_words:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def part_of_speech(self, text):\n",
    "        s = text.split()\n",
    "        nonstop = [word for word in s if word not in self.stop]\n",
    "        pos = [part[1] for part in nltk.pos_tag(nonstop)]\n",
    "        pos = ' '.join(pos)\n",
    "        return pos\n",
    "\n",
    "\n",
    "    def __init__(self):        \n",
    "        df_ycb = pd.read_csv('data/clickbait/clickbait_data.txt', sep=\"\\n\", header=None, names=['text'])\n",
    "        df_ycb['clickbait'] = 1\n",
    "\n",
    "        df_ncb = pd.read_csv('data/clickbait/non_clickbait_data.txt', sep=\"\\n\", header=None, names=['text'])\n",
    "        df_ncb['clickbait'] = 0\n",
    "\n",
    "        df = df_ycb.append(df_ncb, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "        \n",
    "\n",
    "       \n",
    "        self.stop = stopwords.words('english')\n",
    "       \n",
    "        # Creating some latent variables from the data\n",
    "        df['text']     = df['text'].apply(self.process_text)\n",
    "        df['question'] = df['text'].apply(self.question_word)\n",
    "\n",
    "        df['num_words']       = df['text'].apply(lambda x: len(x.split()))\n",
    "        df['part_speech']     = df['text'].apply(self.part_of_speech)\n",
    "        df['num_contract']    = df['text'].apply(self.num_contract)\n",
    "        df['num_stop_words']  = df['text'].apply(self.cnt_stop_words)\n",
    "        df['stop_word_ratio'] = df['num_stop_words']/df['num_words']\n",
    "        df['contract_ratio']  = df['num_contract']/df['num_words']\n",
    "\n",
    "        \n",
    "        df.drop(['num_stop_words','num_contract'], axis=1, inplace=True)\n",
    "\n",
    "        df_train, df_test = train_test_split(df, test_size=0.2, random_state=0)\n",
    "\n",
    "        self.tfidf = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode',\n",
    "                                   analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,5),\n",
    "                                   use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "\n",
    "        X_train_text = self.tfidf.fit_transform(df_train['text'])\n",
    "        X_test_text  = self.tfidf.transform(df_test['text'])\n",
    "\n",
    "        self.cvec = CountVectorizer()\n",
    "\n",
    "        X_train_pos = self.cvec.fit_transform(df_train['part_speech'])\n",
    "        X_test_pos  = self.cvec.transform(df_test['part_speech'])\n",
    "\n",
    "        self.scNoMean = StandardScaler(with_mean=False)  # we pass with_mean=False to preserve the sparse matrix\n",
    "        X_train_pos_sc = self.scNoMean.fit_transform(X_train_pos)\n",
    "        X_test_pos_sc  = self.scNoMean.transform(X_test_pos)\n",
    "\n",
    "        X_train_val = df_train.drop(['clickbait','text','part_speech'], axis=1).values\n",
    "        X_test_val  = df_test.drop(['clickbait','text','part_speech'], axis=1).values\n",
    "\n",
    "        self.sc = StandardScaler()\n",
    "        X_train_val_sc = self.sc.fit(X_train_val).transform(X_train_val)\n",
    "        X_test_val_sc  = self.sc.transform(X_test_val)\n",
    "\n",
    "        y_train = df_train['clickbait'].values\n",
    "        y_test  = df_test['clickbait'].values\n",
    "\n",
    "\n",
    "\n",
    "        X_train = sparse.hstack([X_train_val_sc, X_train_text, X_train_pos_sc]).tocsr()\n",
    "        X_test  = sparse.hstack([X_test_val_sc, X_test_text, X_test_pos_sc]).tocsr()\n",
    "\n",
    "        self.model = LogisticRegression(penalty='l2', C=98.94736842105263)\n",
    "        self.model = self.model.fit(X_train, y_train)\n",
    "        \n",
    "        predicted_LogR = self.model.predict(X_test)\n",
    "        score = metrics.accuracy_score(y_test, predicted_LogR)\n",
    "        print(\"Clickbait Model Trained - accuracy:   %0.6f\" % score)\n",
    "\n",
    "#     predict = model.predict(X_test)\n",
    "#     print(classification_report(y_test, predict))\n",
    "\n",
    "\n",
    "    def predict(self, text):\n",
    "        #creating the dataframe with our text so we can leverage the existing code\n",
    "        dfrme = pd.DataFrame(index=[0], columns=['text'])\n",
    "        dfrme['text'] = text\n",
    "\n",
    "        #processing text\n",
    "        dfrme['text']     = dfrme['text'].apply(self.process_text)\n",
    "\n",
    "        #adding latent variables\n",
    "        dfrme['question'] = dfrme['text'].apply(self.question_word)\n",
    "        dfrme['num_words']       = dfrme['text'].apply(lambda x: len(x.split()))\n",
    "        dfrme['part_speech']     = dfrme['text'].apply(self.part_of_speech)\n",
    "        dfrme['num_contract']    = dfrme['text'].apply(self.num_contract)\n",
    "        dfrme['num_stop_words']  = dfrme['text'].apply(self.cnt_stop_words)\n",
    "        dfrme['stop_word_ratio'] = dfrme['num_stop_words']/dfrme['num_words']\n",
    "        dfrme['contract_ratio']  = dfrme['num_contract']/dfrme['num_words']\n",
    "\n",
    "        #removing latent variables that have high colinearity with other features\n",
    "        dfrme.drop(['num_stop_words','num_contract'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        Xtxt_val  = dfrme.drop(['text','part_speech'], axis=1).values\n",
    "        Xtxt_val_sc  = self.sc.transform(Xtxt_val)\n",
    "\n",
    "        Xtxt_text  = self.tfidf.transform(dfrme['text'])\n",
    "\n",
    "        Xtxt_pos  = self.cvec.transform(dfrme['part_speech'])\n",
    "        Xtxt_pos_sc  = self.scNoMean.transform(Xtxt_pos)\n",
    "        Xtxt  = sparse.hstack([Xtxt_val_sc, Xtxt_text, Xtxt_pos_sc]).tocsr()\n",
    "\n",
    "        predicted = self.model.predict(Xtxt)\n",
    "        predicedProb = self.model.predict_proba(Xtxt)[:,1]\n",
    "        return predicted, predicedProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickbait Model Trained - accuracy:   0.976875\n"
     ]
    }
   ],
   "source": [
    "#Code example\n",
    "clickBait = Clickbait()\n",
    "#print(clickBait.predict(\"Should You bring the money now\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
